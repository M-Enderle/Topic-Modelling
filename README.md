# âœ¨ Topic Analysis Adventure âœ¨

Welcome to my exciting journey through topic modeling with **LDA** and **Top2Vec**! Iâ€™ve explored these two powerhouse tools to unearth hidden themes in text data, and Iâ€™m thrilled to share the ups, downs, and discoveries along the way. Buckle upâ€”letâ€™s dive in! ğŸŒŠ

---

## ğŸ§  LDA (Latent Dirichlet Allocation)

### ğŸ‰ Why Itâ€™s Awesome
- **Multi-Topic Magic**: A single document can wear multiple topic hatsâ€”great for messy, real-world texts! ğŸŒŸ
- **Small Data Superstar**: Performs like a champ even with smaller datasets. ğŸ“Š

### ğŸ› ï¸ Preprocessing Prep
To get LDA ready to roll, I whipped the data into shape using the open-source wizards **spaCy** and **NLTK**. Hereâ€™s what I did:
- ğŸš« Kicked out **stopwords**, **punctuation**, **numbers**, and **special characters**.
- âœ‚ï¸ Transformed words into their root forms with **lemmatization** (e.g., "running" â†’ "run").
- ğŸ”§ Expanded contractions like "I'm" â†’ "I am" for consistency.

Check out the cool **word cloud** I generated to peek at the cleaned-up data!  
![Word Cloud](images/wordcloud.png)

### âš¡ Training Time
I fired up the **LdaMulticore** model from **gensim** for this mission. Itâ€™s lightning-fast âš¡, but youâ€™ve got to tell it how many topics to hunt for upfront. I tweaked and tuned it to see what stuck!

### ğŸ“ˆ Results Reveal
To bring the results to life, I used **pyLDAvis**â€”an open-source gem that creates interactive visualizations. Want to explore the full interactive version? Check it out here: `./results/ldavis_prepared_4.html`.

But hereâ€™s the tea â˜•: The topics wereâ€¦ underwhelming. They lacked that â€œaha!â€ moment. Broad words like "quarter" and "think" hogged the spotlight across all topics, probably because the dataset was too skimpy. More data, please!  
![LDA Visualization](images/LDA.png)

---

## ğŸŒŒ Top2Vec: The Next Frontier

### ğŸ‰ Why It Rocks
- **Embedding Power**: Uses fancy word embeddings for a deeper grasp of meaning. ğŸ§¬
- **Content Whisperer**: Gets the vibe of your documents like a pro. ğŸ¯

### ğŸ› ï¸ Preprocessing? Nope!
Top2Vec is the low-maintenance friend we all needâ€”it handles **all preprocessing** internally. No manual scrubbing required. Just toss in your raw text and let it work its magic! âœ¨

### âš¡ Training Made Simple
Training Top2Vec is a breezeâ€”just one line of code! For an extra boost, I plugged in a **pretrained sentence encoder** (shoutout to the GitHub repoâ€™s sleek implementation!). Itâ€™s fast, efficient, and oh-so-satisfying.  
![Top2Vec Training](images/Top2Vec.png)

### ğŸ“ˆ Results That Wow
Top2Vec delivered the goods! It sniffed out **4 distinct topics**, neatly tied to the 4 companies in my dataset (nailed it!). The separation was crisp, and the topics actually made senseâ€”way more than LDA managed here.  
Peek at these beauties:  
![Topic 2](images/topic_2.png)  
![Topic 3](images/topic_3.png)

---

## ğŸ’­ Final Thoughts
**Top2Vec** stole the show with its slick performance and meaningful results, leaving LDA in the dust this time. But letâ€™s be realâ€”both models were starving for more data. My next move? Feed them a bigger corpus to unlock their full potential! ğŸ½ï¸  
For future adventures, Iâ€™d double down on the Top2Vec approachâ€”itâ€™s got the edge Iâ€™m looking for. ğŸš€

---

## ğŸ–ï¸ Credits
All the code-fu in this project comes from yours truly, **Moritz Enderle**. Want to see the magic behind the curtain? Check out the full codebase on GitHub: [M-Enderle/Topic-Modelling](https://github.com/M-Enderle/Topic-Modelling/). Big thanks to the open-source community for the tools that made this possible! ğŸ™Œ

---

What do you think? Ready to explore topic modeling with me? Letâ€™s chat! ğŸ’¬
